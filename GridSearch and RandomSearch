def randomsrch_gridcv(x,lst):
    
         def evaluate(model, test_features, test_labels):
            predictions = pd.DataFrame(model.predict(test_features))
            predictions[0] = pd.to_numeric(predictions[0])
            test_labels = test_labels.reset_index()
            test_labels = test_labels.drop(['index'], axis= 1)
            test_labels['Volume/HL'] = pd.to_numeric(test_labels['Volume/HL'])
            errors = abs(predictions[0] - test_labels['Volume/HL'])/abs(test_labels['Volume/HL'])
            mape = (sum(errors)/len(test_labels))*100
            accuracy = 100 - mape    
            return accuracy
        
         xx_train = lst[x][0]
         xx_test = lst[x][1]
         yy_train = lst[x][2]
         yy_test = lst[x][3]
         X_train = xx_train.drop(['Time Period Long'], axis = 1)
         X_test = xx_test.drop(['Time Period Long'], axis = 1)
         x_time_test = xx_test.filter(items = ['Time Period Long'], axis = 1)
         y_train = yy_train.drop(['Time Period Long'], axis = 1)
         y_test = yy_test.drop(['Time Period Long'], axis = 1)
            
         # Number of trees in random forest
         n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)]
         # Number of features to consider at every split
         max_features = ['auto', 'sqrt']
         # Maximum number of levels in tree
         max_depth = [int(x) for x in np.linspace(3, int(0.6*len(X_train)), num =int(int(0.6*len(X_train))/3) )]
         if len(X_train) < 3:
             maxdepthr = [int(x) for x in range(1,int(len(X_train)))]
         else: 
             maxdepthr = max_depth
         print(maxdepthr)
    #max_depth.append(None)
         # Minimum number of samples required to split a node
         min_samples_split = [3, 4, 5]
            # Minimum number of samples required at each leaf node
         min_samples_leaf = [2, 3, 4]
         # Method of selecting samples for training each tree
         bootstrap = [True, False]
            
         # Create the random grid
         random_grid = {'n_estimators': n_estimators,
                        'max_features': max_features,
                        'max_depth': maxdepthr,
                        'min_samples_split': min_samples_split,
                        'min_samples_leaf': min_samples_leaf,
                        'bootstrap': bootstrap}
                
         rf = RandomForestRegressor()
         # Random search of parameters, using 3 fold cross validation, search across 100 different combinations, and use all available cores
         rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
         # Fit the random search model
         rf_random.fit(X_train, y_train)
         best_set = rf_random.best_params_
         best_random = rf_random.best_estimator_
         random_accuracy = evaluate(best_random, X_test, y_test)
         print(best_set)
         maxdepth = [int(x) for x in range(int(best_set['max_depth']),int(max(maxdepthr)))]
         if not maxdepth:
             maxdepth = [int(x) for x in np.linspace(3, int(0.6*len(X_train)), num =int(int(0.6*len(X_train))/3) )]
         else: maxdepth = maxdepth
         param_grid = {
             'max_depth': maxdepth,
             'max_features': [str(best_set['max_features'])],
    #            'min_samples_leaf': [int(x) for x in range(int(best_set['min_samples_leaf']),max(min_samples_leaf))],
             'min_samples_leaf': [int(best_set['min_samples_leaf'])-1,int(best_set['min_samples_leaf']),int(best_set['min_samples_leaf'])+1],
    #             'min_samples_split': [int(x) for x in range(int(best_set['min_samples_split']),max(min_samples_split))],
             'min_samples_split': [int(best_set['min_samples_split'])-1,int(best_set['min_samples_split']),int(best_set['min_samples_split'])+1],
             'n_estimators': [int(best_set['n_estimators'])-9,int(best_set['n_estimators']),int(best_set['n_estimators'])+9]
                 }
             
         rf_hyp = RandomForestRegressor()
    #        # Instantiate the grid search model
         grid_search = GridSearchCV(estimator = rf_hyp, param_grid = param_grid, 
                                      cv = 3, n_jobs = -1, verbose = 2)
         grid_search.fit(X_train, y_train)
         grid_search.best_params_
            
         best_grid = grid_search.best_estimator_
         grid_accuracy = evaluate(best_grid, X_test, y_test)
         predictions = pd.DataFrame(best_grid.predict(X_test))
         predicts = pd.concat([x_time_test,predictions], axis =1)
         actuals = pd.concat([yy_test,y_test], axis =1)
     
         return [random_accuracy,grid_accuracy,actuals,predicts]
